---
layout: post
title: 神经网络常见激活函数总结
mathjax: true
categories: Knowledge
tags: [激活函数]
keywords: relu,sigmod,tanh
description: 总结神经网络中常见的激活函数
---

> 神经网络的激活函数是使神经网络模型变得非线性的关键一环，非线性的模型才有能力拟合复杂的模式特征。本文总结常见的激活函数，展示其图像、公式及其导数，并分析其优缺点，相应的使用场景等。

---

参考资料：

- [从ReLU到GELU，一文概览神经网络的激活函数](https://baijiahao.baidu.com/s?id=1653421414340022957&wfr=spider&for=pc )

# ReLU

## 公式及其导数

$$
\operatorname{Re} {\text{LU}}(x) = \left\{ {\begin{array}{*{20}{c}}
   {x,if\;x > 0}  \\
   {0,if\;x < 0}  \\

 \end{array} } \right.,
 \quad 
 \operatorname{Re} {\text{LU'}}(x) = \left\{ {\begin{array}{*{20}{c}}
   {1,if\;x > 0}  \\
   {0,if\;x < 0}  \\

 \end{array} } \right.
$$

<img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/relu.png" style="zoom:80%" />

<img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/relu_derivative.png" style="zoom:80%" />

## 优缺点

xxx





<img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/which_dir/xxx.png" style="zoom:80%" />

