---
layout: post
title: 神经网络常见激活函数总结
mathjax: true
categories: Knowledge
tags: [激活函数]
keywords: relu,sigmod,tanh
description: 总结神经网络中常见的激活函数
---

> 神经网络的激活函数是使神经网络模型变得非线性的关键一环，非线性的模型才有能力拟合复杂的模式特征。本文总结常见的激活函数，展示其图像、公式及其导数，并分析其优缺点，相应的使用场景等。

---

> 参考资料：
>
> - [从ReLU到GELU，一文概览神经网络的激活函数](https://baijiahao.baidu.com/s?id=1653421414340022957&wfr=spider&for=pc )
> - [神经网络中的常用激活函数和导数](https://blog.csdn.net/lw_power/article/details/90291928 )
> - [常用激活函数](https://blog.csdn.net/hfutdog/article/details/96483480 )

---

# Sigmoid

## 公式及其导数

 <center> 
     <img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/sigmoid.jpg" style="zoom:80%" /><img 
</center>

其中 $$\sigma (x)$$ 表示 sigmoid 本身。

 <center> 
     <img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/sigmoid.png" style="zoom:80%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/sigmoid_derivative.png" style="zoom:80%" />
</center>

## 优缺点

**Sigmoid的优点**：

- Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定
- 求导容易

**Sigmoid的缺点**：

- 幂运算，计算成本高
- 容易出现梯度弥散（反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练）
- 不是以0为中心，为导致收敛速度下降（具体解释可以参考[谈谈激活函数以零为中心的问题](https://liam.page/2018/04/17/zero-centered-active-function/)）



# Tanh

Tanh 跟 Sigmoid 可以说是同一种激活函数，只是值域不一样。但它的输出均值为0，这使得它的收敛速度要比sigmoid 快，减少了迭代更新的次数。 

## 公式及其导数

$$
Tanh(x) = \frac{e^z - {e^{-z}}}
{e^z + e^{-z}} = 2Sigmoid(2x) - 1,\quad Tanh'(x) = 1 - Tanh(x)^2
$$

 <center> 
     <img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/tanh.png" style="zoom:80%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/tanh_derivative.png" style="zoom:80%" />
</center>

## 优缺点

**Tanh的优点**

- 输出以0为中心。
- 比sigmoid函数训练时收敛更快。

**Tanh的缺点**

- 仍然是饱和函数，没有解决梯度消失问题。



# ReLU

 这个激活函数只是简单地将大于0的部分保留，将小于0的部分变成 0 

## 公式及其导数

$$
\operatorname{ReLU}(x) = \left\{ {\begin{array}{*{20}{c}}   {x,if\;x > 0}  \\   {0,if\;x < 0}  \\ \end{array} } \right., \quad  \operatorname{ReLU'}(x) = \left\{ {\begin{array}{*{20}{c}}   {1,if\;x > 0}  \\   {0,if\;x < 0}  \\ \end{array} } \right.
$$

 <center> 
     <img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/relu.png" style="zoom:80%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/relu_derivative.png" style="zoom:80%" />
</center>

## 优缺点

**ReLU 的优点**:

- x 大于0时，其导数恒为1，这样就不会存在梯度消失的问题，即解决了部分梯度消失问题 
- 计算导数非常快，只需要判断 x 是大于0，还是小于0
- 收敛速度远远快于前面的 Sigmoid 和 Tanh函数

**ReLU 的缺点**:

-  当x<0时，出现梯度消失问题。此时相当于神经元死亡。 即某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。因为当x 小于等于0时输出恒为0，如果某个神经元的输出总是满足小于等于0 的话，那么它将无法进入计算。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用 MSRA 初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。
-  ReLU 不能避免梯度爆炸问题。 



# Leaky ReLU

## 公式及其导数

$$
{\text{LeakyReLU(}}x{\text{) = }}\left\{ {\begin{array}{*{20}{c}}
   {x,\quad \quad x > 0}  \\
   {\alpha  * x,\quad x \leqslant 0}  \\

 \end{array} } \right.,\quad {\text{LeakyReLU'(}}x{\text{) = }}\left\{ {\begin{array}{*{20}{c}}
   {1,\quad x > 0}  \\
   {\alpha ,\quad x \leqslant 0}  \\

 \end{array} } \right.
$$



<center> 
<img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/leaky_relu.png" style="zoom:80%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/leaky_relu_derivative.png" style="zoom:80%" />
</center>

## 优缺点

 Leaky_ReLU 主要是解决了 ReLU 神经元死掉的问题。依然不能避免梯度爆炸的问题。 



# Summary

## 选择正确的激活函数

这么多激活函数需要在什么时候使用什么呢？这里并没有特定的规则。但是根据这些函数的特征，我们也可以总结一个比较好的使用规律或者使用经验，使得网络可以更加容易且更快的收敛。

- Sigmoid函数以及它们的联合通常在分类器的中有更好的效果
- 由于梯度崩塌的问题，在某些时候需要避免使用Sigmoid和Tanh激活函数
- ReLU函数是一种常见的激活函数，在目前使用是最多的
- 如果遇到了一些死的神经元，我们可以使用Leaky ReLU函数
- 记住，ReLU永远只在隐藏层中使用
- 根据经验，我们一般可以从ReLU激活函数开始，但是如果ReLU不能很好的解决问题，再去尝试其他的激活函数



# 实验

最后我使用不同的激活函数做了一个mnist分类的实验。实验结果如下图所示：

<center> 
<img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/train_loss.png" style="zoom:60%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/train_acc.png" style="zoom:60%" />
</center>

<center> 
<img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/test_loss.png" style="zoom:60%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/test_acc.png" style="zoom:60%" />
</center>

其中 Sigmoid 作为激活函数实验了两次，一次学习率为 1e-3, 一次为 1e-2, 可以看出Sigmoid 作为激活函数是收敛速度很慢。ReLU 比其他激活函数稍微的好一点点，其他的对比并不明显。

代码链接： https://github.com/huangtao36/huangtao36.github.io/tree/master/assets/code 