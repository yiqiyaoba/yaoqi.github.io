---
layout: post
title: 神经网络常见激活函数总结
mathjax: true
categories: Knowledge
tags: [激活函数]
keywords: relu,sigmod,tanh
description: 总结神经网络中常见的激活函数
---

> 神经网络的激活函数是使神经网络模型变得非线性的关键一环，非线性的模型才有能力拟合复杂的模式特征。本文总结常见的激活函数，展示其图像、公式及其导数，并分析其优缺点，相应的使用场景等。

---

参考资料：

- [从ReLU到GELU，一文概览神经网络的激活函数](https://baijiahao.baidu.com/s?id=1653421414340022957&wfr=spider&for=pc )

# ReLU

 这个激活函数只是简单地将大于0的部分保留，将小于0的部分变成 0 

## 公式及其导数

$$
\operatorname{ReLU}(x) = \left\{ {\begin{array}{*{20}{c}}
   {x,if\;x > 0}  \\
   {0,if\;x < 0}  \\
 \end{array} } \right.,
 \quad 
 \operatorname{ReLU'}(x) = \left\{ {\begin{array}{*{20}{c}}
   {1,if\;x > 0}  \\
   {0,if\;x < 0}  \\
 \end{array} } \right.
$$

 <center> 
     <img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/relu.png" style="zoom:80%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/relu_derivative.png" style="zoom:80%" />
</center>

## 优缺点

**ReLU 的优点**:

- x 大于0时，其导数恒为1，这样就不会存在梯度消失的问题
- 计算导数非常快，只需要判断 x 是大于0，还是小于0
- 收敛速度远远快于前面的 Sigmoid 和 Tanh函数

**ReLU 的缺点**:

- Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。因为当x 小于等于0时输出恒为0，如果某个神经元的输出总是满足小于等于0 的话，那么它将无法进入计算。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用 MSRA 初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

# Sigmoid

## 公式及其导数

$$
xx
$$

 <center> 
     <img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/sigmod.png" style="zoom:80%" /><img src="https://raw.githubusercontent.com/huangtao36/huangtao36.github.io/master/_posts/2020-04-03-神经网络常见激活函数总结/assert/sigmod_derivative.png" style="zoom:80%" />
</center>

## 优缺点

**Sigmoid的优点**：

- Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定
- 求导容易

**Sigmoid的缺点**：

- 幂运算，计算成本高
- 容易出现梯度弥散（反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练）
- 不是以0为中心，为导致收敛速度下降（具体解释可以参考[谈谈激活函数以零为中心的问题](https://liam.page/2018/04/17/zero-centered-active-function/)）



————No Done Yet!